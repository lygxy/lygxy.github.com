```
URL:
https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/1_tensor_tutorial.ipynb
```

##### 7

```python
x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
```

```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 0.5691, -2.0126, -0.4064],
        [-0.0863,  0.4692, -1.1209],
        [-1.1177, -0.5764, -0.5363],
        [-0.4390,  0.6688,  0.0889],
        [ 1.3334, -1.1600,  1.8457]])
```

##### 8获取size

```python
print(x.size())
```

```
torch.Size([5, 3])
```

#### Note

``torch.Size`` 返回值是 tuple类型, 所以它支持tuple类型的所有操作.

##### 9操作 - 加法运算

```python
# 加法1
y = torch.rand(5, 3)
print(x + y)
```

```
tensor([[ 0.7808, -1.4388,  0.3151],
        [-0.0076,  1.0716, -0.8465],
        [-0.8175,  0.3625, -0.2005],
        [ 0.2435,  0.8512,  0.7142],
        [ 1.4737, -0.8545,  2.4833]])
```

```python
# 加法2
print(torch.add(x, y))
```

```
tensor([[ 0.7808, -1.4388,  0.3151],
        [-0.0076,  1.0716, -0.8465],
        [-0.8175,  0.3625, -0.2005],
        [ 0.2435,  0.8512,  0.7142],
        [ 1.4737, -0.8545,  2.4833]])
```

```PYTHON
# 提供输出tensor作为参数
result = torch.empty(5, 3)
torch.add(x, y, out=result)
print(result)
```

```
tensor([[ 0.7808, -1.4388,  0.3151],
        [-0.0076,  1.0716, -0.8465],
        [-0.8175,  0.3625, -0.2005],
        [ 0.2435,  0.8512,  0.7142],
        [ 1.4737, -0.8545,  2.4833]])
```

##### 替换

```
# adds x to y
y.add_(x)
print(y)
```

```
tensor([[ 0.7808, -1.4388,  0.3151],
        [-0.0076,  1.0716, -0.8465],
        [-0.8175,  0.3625, -0.2005],
        [ 0.2435,  0.8512,  0.7142],
        [ 1.4737, -0.8545,  2.4833]])
```

#### Note

任何 以``_`` 结尾的操作都会用结果替换原变量. 例如: ``x.copy_(y)``, ``x.t_()``, 都会改变 ``x``.

##### 你可以使用与NumPy索引方式相同的操作来进行对张量的操作

```python
print(x[:, 1])
```

```
tensor([-2.0126,  0.4692, -0.5764,  0.6688, -1.1600])
```

##### `torch.view`: 可以改变张量的维度和大小

***译者注：torch.view 与Numpy的reshape\***

```
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
```

```
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

##### 如果你有只有一个元素的张量，使用`.item()`来得到Python数据类型的数值

```python
x = torch.randn(1)
print(x)
print(x.item())
```

```
tensor([-0.2368])
-0.23680149018764496
```





### NumPy转换

将一个Torch Tensor转换为NumPy数组是一件轻松的事，反之亦然。

Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。

将一个Torch Tensor转换为NumPy数组

```
a = torch.ones(5)
print(a)
```

```
tensor([1., 1., 1., 1., 1.])
```

```python
b = a.numpy()
print(b)
```

```
[1. 1. 1. 1. 1.]
```



观察numpy数组的值是如何改变的。

```
a.add_(1)
print(a)
print(b)
```

```
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
```

NumPy Array 转化成 Torch Tensor

使用from_numpy自动转化

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)
```

```
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
```

所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到 NumPy 的转换.



## CUDA 张量

使用`.to` 方法 可以将Tensor移动到任何设备中

In [20]:

```python
# is_available 函数判断是否有cuda可以使用
# ``torch.device``将张量移动到指定的设备中
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA 设备对象
    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量
    x = x.to(device)                       # 或者直接使用``.to("cuda")``将张量移动到cuda中
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # ``.to`` 也会对变量的类型做更改
tensor([0.7632], device='cuda:0')
tensor([0.7632], dtype=torch.float64)
```



## Autograd:自动求导机制

`import torch`

创建一个张量并设置require_grad=True用来追踪他的计算历史

```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
```

```
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
```



对张量进行操作

```python
y = x + 2
print(y)
```

```
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
```

结果`y`已经被计算出来了，所以，`grad_fn`已经被自动生成了。

```python
print(y.grad_fn)
```

```
<AddBackward0 object at 0x000002004F7CC248>
```



对y进行一个操作

```python
z = y * y * 3
out = z.mean()

print(z, out)
```

```
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)
```

`.requires_grad_( ... )` 可以改变现有张量的 `requires_grad`属性。 如果没有指定的话，默认输入的flag是 `False`。

```python
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
False
True
<SumBackward0 object at 0x000002004F7D5608>
```

##### CSDN上对于.mean的注释代码

```python
x=torch.arange(15).view(5,3)
x_mean=torch.mean(x,dim=0,keepdim=True)
x_mean0=torch.mean(x,dim=1,keepdim=True)
print('before bn:')
print(x)
print('x_mean:')
print(x_mean)
print('x_mean0:')
print(x_mean0)
```

运行结果：

```
before bn:
 
  0   1   2
  3   4   5
  6   7   8
  9  10  11
 12  13  14
[torch.FloatTensor of size 5x3]
 
x_mean:
 
 6  7  8
[torch.FloatTensor of size 1x3]
 
x_mean0:
 
  1
  4
  7
 10
 13
[torch.FloatTensor of size 5x1]
```

##### 知乎上view的用法

在pytorch中view函数的作用为重构张量的维度，相当于numpy中resize（）的功能，但是用法可能不太一样。如下例所示

```
>>> tt1=torch.tensor([-0.3623, -0.6115,  0.7283,  0.4699,  2.3261,  0.1599])
>>> result=tt1.view(3,2)
>>> result
tensor([[-0.3623, -0.6115],
        [ 0.7283,  0.4699],
        [ 2.3261,  0.1599]])
```

torch.view(参数a，参数b，...)
在上面例子中参数a=3和参数b=2决定了将一维的tt1重构成3x2维的张量

有的时候会出现torch.view(-1)或者torch.view(参数a，-1)这种情况

```text
>>> import torch
>>> tt2=torch.tensor([[-0.3623, -0.6115],
...         [ 0.7283,  0.4699],
...         [ 2.3261,  0.1599]])
>>> result=tt2.view(-1)
>>> result
tensor([-0.3623, -0.6115,  0.7283,  0.4699,  2.3261,  0.1599])
```

由上面的案例可以看到，如果是torch.view(-1)，则原张量会变成一维的结构



```text
>>> import torch
>>> tt3=torch.tensor([[-0.3623, -0.6115],
...         [ 0.7283,  0.4699],
...         [ 2.3261,  0.1599]])
>>> result=tt3.view(2,-1)
>>> result
tensor([[-0.3623, -0.6115,  0.7283],
        [ 0.4699,  2.3261,  0.1599]])
```

由上面的案例可以看到，如果是torch.view(参数a，-1)，则表示在参数b未知，参数a已知的情况下自动补齐列向量长度，在这个例子中a=2，tt3总共由6个元素，则b=6/2=3。

```
评论：
1.所以，view就相当于是reshape
2..view return a new tensor mean while .reshape do reshape the original tensor
```

### 神经网络 - Neural Networks

使用torch.nn包来构建神经网络。

上一讲已经讲过了`autograd`，`nn`包依赖`autograd`包来定义模型并求导。 一个`nn.Module`包含各个层和一个`forward(input)`方法，该方法返回`output`。

例如：

![img](https://camo.githubusercontent.com/132535d9a83564868524f60e8e1ae5e27990709e/68747470733a2f2f7079746f7263682e6f72672f7475746f7269616c732f5f696d616765732f6d6e6973742e706e67)

它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。

神经网络的典型训练过程如下：

1. 定义包含一些可学习的参数(或者叫权重)神经网络模型；
2. 在数据集上迭代；
3. 通过神经网络处理输入；
4. 计算损失(输出结果和正确值的差值大小)；
5. 将梯度反向传播回网络的参数；
6. 更新网络的参数，主要使用如下简单的更新原则： `weight = weight - learning_rate * gradient`

##### 定义网络

开始定义一个网络:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
```

`url:https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/3_neural_networks_tutorial.ipynb`

```
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
```

