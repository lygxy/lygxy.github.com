`https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.1.1.pytorch-basics-tensor.ipynb`

### PyTorch基础

```python
# 首先要引入相关的包
import torch
import numpy as np
#打印一下版本
torch.__version__
```

##### 张量（Tensor）

张量的英文是Tensor，它是PyTorch里面基础的运算单位，与Numpy的ndarray相同都表示的是一个多维的矩阵。 与ndarray的最大区别就是，PyTorch的Tensor可以在 GPU 上运行，而 numpy 的 ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。

下面我们生成一个简单的张量

```python
x = torch.rand(2, 3)
x
```

```python
tensor([[0.6904, 0.7419, 0.8010],
        [0.1722, 0.2442, 0.8181]])
```

以上生成了一个，2行3列的的矩阵，我们看一下他的大小：

```python
# 可以使用与numpy相同的shape属性查看
print(x.shape)
# 也可以使用size()函数，返回的结果都是相同的
print(x.size())
```

```python
torch.Size([2, 3])
torch.Size([2, 3])
```

张量（Tensor）是一个定义在一些向量空间和一些对偶空间的笛卡儿积上的多重线性映射，其坐标是|n|维空间内，有|n|个分量的一种量， 其中每个分量都是坐标的函数， 而在坐标变换时，这些分量也依照某些规则作线性变换。r称为该张量的秩或阶（与矩阵的秩和阶均无关系）。 (来自百度百科)



下面我们来生成一些多维的张量：

```python
y=torch.rand(2,3,4,5)
print(y.size())
y
```

```
torch.Size([2, 3, 4, 5])
```

```python
tensor([[[[0.9071, 0.0616, 0.0006, 0.6031, 0.0714],
          [0.6592, 0.9700, 0.0253, 0.0726, 0.5360],
          [0.5416, 0.1138, 0.9592, 0.6779, 0.6501],
          [0.0546, 0.8287, 0.7748, 0.4352, 0.9232]],

         [[0.0730, 0.4228, 0.7407, 0.4099, 0.1482],
          [0.5408, 0.9156, 0.6554, 0.5787, 0.9775],
          [0.4262, 0.3644, 0.1993, 0.4143, 0.5757],
          [0.9307, 0.8839, 0.8462, 0.0933, 0.6688]],

         [[0.4447, 0.0929, 0.9882, 0.5392, 0.1159],
          [0.4790, 0.5115, 0.4005, 0.9486, 0.0054],
          [0.8955, 0.8097, 0.1227, 0.2250, 0.5830],
          [0.8483, 0.2070, 0.1067, 0.4727, 0.5095]]],


        [[[0.9438, 0.2601, 0.2885, 0.5457, 0.7528],
          [0.2971, 0.2171, 0.3910, 0.1924, 0.2570],
          [0.7491, 0.9749, 0.2703, 0.2198, 0.9472],
          [0.1216, 0.6647, 0.8809, 0.0125, 0.5513]],

         [[0.0870, 0.6622, 0.7252, 0.4783, 0.0160],
          [0.7832, 0.6050, 0.7469, 0.7947, 0.8052],
          [0.1755, 0.4489, 0.0602, 0.8073, 0.3028],
          [0.9937, 0.6780, 0.9425, 0.0059, 0.0451]],

         [[0.3851, 0.8742, 0.5932, 0.4899, 0.8354],
          [0.8577, 0.3705, 0.0229, 0.7097, 0.7557],
          [0.1505, 0.3527, 0.0843, 0.0088, 0.8741],
          [0.6041, 0.8797, 0.6189, 0.9495, 0.1479]]]])
```

在同构的意义下，第零阶张量 （r = 0） 为标量 （Scalar），第一阶张量 （r = 1） 为向量 （Vector）， 第二阶张量 （r = 2） 则成为矩阵 （Matrix），第三阶以上的统称为多维张量。

其中要特别注意的就是标量，我们先生成一个标量：

```python
#我们直接使用现有数字生成
scalar =torch.tensor(3.1433223)
print(scalar)
#打印标量的大小
scalar.size()
```

```
tensor(3.1433)
```

```
torch.Size([])
```

对于标量，我们可以直接使用 .item() 从中取出其对应的python对象的数值

```python
scalar.item()
```

```
3.143322229385376
```

特别的：如果张量中只有一个元素的tensor也可以调用`tensor.item`方法

```python
tensor = torch.tensor([3.1433223]) 
print(tensor)
tensor.size()
```

```
tensor([3.1433])
```

```
torch.Size([1])
```

```python
tensor.item()
```

3.143322229385376



##### 基本类型

Tensor的基本数据类型有五种：

- 32位浮点型：torch.FloatTensor。 (默认)
- 64位整型：torch.LongTensor。
- 32位整型：torch.IntTensor。
- 16位整型：torch.ShortTensor。
- 64位浮点型：torch.DoubleTensor。

除以上数字类型外，还有 byte和chart型

```python
long=tensor.long()
long
```

tensor([3])

```python
half=tensor.half()
half
```

tensor([3.1426], dtype=torch.float16)

```python
int_t=tensor.int()
int_t
```

tensor([3], dtype=torch.int32)



```python
flo = tensor.float()
flo
```

tensor([3.1433])

```
short = tensor.short()
short
```

Out[13]:

```
tensor([3], dtype=torch.int16)
```

In [14]:

```
ch = tensor.char()
ch
```

Out[14]:

```
tensor([3], dtype=torch.int8)
```

In [15]:

```
bt = tensor.byte()
bt
```

Out[15]:

```
tensor([3], dtype=torch.uint8)
```

#### Numpy转换

使用numpy方法将Tensor转为ndarray

In [16]:

```python
a = torch.randn((3, 2))
# tensor转化为numpy
numpy_a = a.numpy()
print(numpy_a)
[[ 0.46819344  1.3774964 ]
 [ 0.9491934   1.4543315 ]
 [-0.42792308  0.99790514]]
```

numpy转化为Tensor

In [17]:

```
torch_a = torch.from_numpy(numpy_a)
torch_a
```

Out[17]:

```
tensor([[ 0.4682,  1.3775],
        [ 0.9492,  1.4543],
        [-0.4279,  0.9979]])
```

**Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。**

##### 设备间转换

一般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持

In [18]:

```
cpu_a=torch.rand(4, 3)
cpu_a.type()
```

Out[18]:

```
'torch.FloatTensor'
```

In [19]:

```
gpu_a=cpu_a.cuda()
gpu_a.type()
```

Out[19]:

```
'torch.cuda.FloatTensor'
```

使用.cpu方法将tensor移动到cpu

In [20]:

```
cpu_b=gpu_a.cpu()
cpu_b.type()
```

Out[20]:

```
'torch.FloatTensor'
```

如果我们有多GPU的情况，可以使用to方法来确定使用那个设备，这里只做个简单的实例：

In [21]:

```
#使用torch.cuda.is_available()来确定是否有cuda设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
#将tensor传送到设备
gpu_b=cpu_b.to(device)
gpu_b.type()
cuda
```

Out[21]:

```
'torch.cuda.FloatTensor'
```

##### 初始化

Pytorch中有许多默认的初始化方法可以使用

In [22]:

```
# 使用[0,1]均匀分布随机初始化二维数组
rnd = torch.rand(5, 3)
rnd
```

Out[22]:

```
tensor([[0.3804, 0.0297, 0.5241],
        [0.4111, 0.8887, 0.4642],
        [0.7302, 0.5913, 0.7182],
        [0.3048, 0.8055, 0.2176],
        [0.6195, 0.1620, 0.7726]])
```

In [23]:

```
##初始化，使用1填充
one = torch.ones(2, 2)
one
```

Out[23]:

```
tensor([[1., 1.],
        [1., 1.]])
```

In [24]:

```
##初始化，使用0填充
zero=torch.zeros(2,2)
zero
```

Out[24]:

```
tensor([[0., 0.],
        [0., 0.]])
```

In [25]:

```
#初始化一个单位矩阵，即对角线为1 其他为0
eye=torch.eye(2,2)
eye
```

Out[25]:

```python
tensor([[1., 0.],
        [0., 1.]])
```

##### 常用方法

PyTorch中对张量的操作api 和 NumPy 非常相似，如果熟悉 NumPy 中的操作，那么他们二者基本是一致的：

In [26]:

```
x = torch.randn(3, 3)
print(x)
tensor([[ 0.6922, -0.4824,  0.8594],
        [ 0.4509, -0.8155, -0.0368],
        [ 1.3533,  0.5545, -0.0509]])
```

In [27]:

```
# 沿着行取最大值
max_value, max_idx = torch.max(x, dim=1)
print(max_value, max_idx)
tensor([0.8594, 0.4509, 1.3533]) tensor([2, 0, 0])
```

In [28]:

```
# 每行 x 求和
sum_x = torch.sum(x, dim=1)
print(sum_x)
tensor([ 1.0692, -0.4014,  1.8568])
```

In [29]:

```
y=torch.randn(3, 3)
z = x + y
print(z)
tensor([[-0.3821, -2.6932, -1.3884],
        [ 0.7468, -0.7697, -0.0883],
        [ 0.7688, -1.3485,  0.7517]])
```



正如官方60分钟教程中所说，以_为结尾的，均会改变调用值

In [30]:

```
# add 完成后x的值改变了
x.add_(y)
print(x)
tensor([[-0.3821, -2.6932, -1.3884],
        [ 0.7468, -0.7697, -0.0883],
        [ 0.7688, -1.3485,  0.7517]])
```

张量的基本操作都介绍的的差不多了，下一章介绍PyTorch的自动求导机制



`https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.1.2-pytorch-basics-autograd.ipynb`

```python
import torch
torch.__version__
```

### 使用PyTorch计算梯度数值

```
PyTorch的Autograd模块实现了深度学习的算法中的向传播求导数，在张量（Tensor类）上的所有操作，Autograd都能为他们自动提供微分，简化了手动计算导数的复杂过程。

在0.4以前的版本中，Pytorch使用Variable类来自动计算所有的梯度Variable类主要包含三个属性： data：保存Variable所包含的Tensor；grad：保存data对应的梯度，grad也是个Variable，而不是Tensor，它和data的形状一样；grad_fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度。

从0.4起， Variable 正式合并入Tensor类，通过Variable嵌套实现的自动微分功能已经整合进入了Tensor类中。虽然为了代码的兼容性还是可以使用Variable(tensor)这种方式进行嵌套，但是这个操作其实什么都没做。

所以，以后的代码建议直接使用Tensor类进行操作，因为官方文档中已经将Variable设置成过期模块。

要想通过Tensor类本身就支持了使用autograd功能，只需要设置.requires_grad=True

Variable类中的的grad和grad_fn属性已经整合进入了Tensor类中
```

##### Autograd

```
在张量创建时，通过设置 requires_grad 标识为Ture来告诉Pytorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算
```

In [2]:

```
x = torch.rand(5, 5, requires_grad=True)
x
```

Out[2]:

```
tensor([[0.0403, 0.5633, 0.2561, 0.4064, 0.9596],
        [0.6928, 0.1832, 0.5380, 0.6386, 0.8710],
        [0.5332, 0.8216, 0.8139, 0.1925, 0.4993],
        [0.2650, 0.6230, 0.5945, 0.3230, 0.0752],
        [0.0919, 0.4770, 0.4622, 0.6185, 0.2761]], requires_grad=True)
```

In [3]:

```
y = torch.rand(5, 5, requires_grad=True)
y
```

Out[3]:

```
tensor([[0.2269, 0.7673, 0.8179, 0.5558, 0.0493],
        [0.7762, 0.9242, 0.2872, 0.0035, 0.4197],
        [0.4322, 0.5281, 0.9001, 0.7276, 0.3218],
        [0.5123, 0.6567, 0.9465, 0.0475, 0.9172],
        [0.9899, 0.9284, 0.5303, 0.1718, 0.3937]], requires_grad=True)
```

PyTorch会自动追踪和记录对与张量的所有操作，当计算完成后调用.backward()方法自动计算梯度并且将计算结果保存到grad属性中。

In [4]:

```python
z=torch.sum(x+y)
z
```

Out[4]:

```
tensor(25.6487, grad_fn=<SumBackward0>)
```

在张量进行操作后，grad_fn已经被赋予了一个新的函数，这个函数引用了一个创建了这个Tensor类的Function对象。 Tensor和Function互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个.grad_fn属性，如果这个张量是用户手动创建的那么这个张量的grad_fn是None。



下面我们来调用反向传播函数，计算其梯度

##### 简单的自动求导

```python
z.backward()
print(x.grad, y.grad)
```

```
tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])
```

**如果Tensor类表示的是一个标量（即它包含一个元素的张量），则不需要为backward()指定任何参数，但是如果它有更多的元素，则需要指定一个gradient参数，它是形状匹配的张量。 以上的 `z.backward()`相当于是`z.backward(torch.tensor(1.))`的简写。 这种参数常出现在图像分类中的单标签分类，输出一个标量代表图像的标签。**

##### 复杂的自动求导

In [6]:

```
x = torch.rand(5, 5, requires_grad=True)
y = torch.rand(5, 5, requires_grad=True)
z= x**2+y**3
z
```

Out[6]:

```
tensor([[3.3891e-01, 4.9468e-01, 8.0797e-02, 2.5656e-01, 2.9529e-01],
        [7.1946e-01, 1.6977e-02, 1.7965e-01, 3.2656e-01, 1.7665e-01],
        [3.1353e-01, 2.2096e-01, 1.2251e+00, 5.5087e-01, 5.9572e-02],
        [1.3015e+00, 3.8029e-01, 1.1103e+00, 4.0392e-01, 2.2055e-01],
        [8.8726e-02, 6.9701e-01, 8.0164e-01, 9.7221e-01, 4.2239e-04]],
       grad_fn=<AddBackward0>)
```

In [7]:

```
#我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数，这里我们用ones_like函数根据x生成一个张量
z.backward(torch.ones_like(x))
print(x.grad)
tensor([[0.2087, 1.3554, 0.5560, 1.0009, 0.9931],
        [1.2655, 0.1223, 0.8008, 1.1127, 0.7261],
        [1.1052, 0.2579, 1.8006, 0.1544, 0.3646],
        [1.8855, 1.2296, 1.9061, 0.9313, 0.0648],
        [0.5952, 1.6190, 0.8430, 1.9213, 0.0322]])
```



我们可以使用with torch.no_grad()上下文管理器临时禁止对已设置requires_grad=True的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到，例如：

In [8]:

```
with torch.no_grad():
    print((x +y*2).requires_grad)
False
```

使用.no_grad()进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。

##### Autograd过程解析

```
为了说明Pytorch的自动求导原理，我们来尝试分析一下PyTorch的源代码，虽然Pytorch的 Tensor和 TensorBase都是使用CPP来实现的，但是可以使用一些Python的一些方法查看这些对象在Python的属性和状态。 Python的 dir() 返回参数的属性、方法列表。z是一个Tensor变量，看看里面有哪些成员变量。
```

```python
dir(z)
```

未完待续...